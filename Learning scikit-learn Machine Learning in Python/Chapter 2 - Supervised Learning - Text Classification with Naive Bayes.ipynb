{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5248bc3d",
   "metadata": {},
   "source": [
    "# Learning Scikit-learn: Machine Learning in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ad0dc2",
   "metadata": {},
   "source": [
    "## Notebook for Chapter 2: Supervised Learning - Text Classification with Na√Øve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40dc8768",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c53e76",
   "metadata": {},
   "source": [
    "Import the newsgroup Dataset, explore its structure and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85ba6631",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f872fe22",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "news = fetch_20newsgroups(subset='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f40611af",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8a95450",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> <class 'numpy.ndarray'> <class 'list'>\n",
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "18846\n",
      "18846\n"
     ]
    }
   ],
   "source": [
    "print (type(news.data), type(news.target), type(news.target_names))\n",
    "print (news.target_names)\n",
    "print (len(news.data))\n",
    "print (len(news.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2792caf4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\n",
      "Subject: Pens fans reactions\n",
      "Organization: Post Office, Carnegie Mellon, Pittsburgh, PA\n",
      "Lines: 12\n",
      "NNTP-Posting-Host: po4.andrew.cmu.edu\n",
      "\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      "\n",
      "10 rec.sport.hockey\n"
     ]
    }
   ],
   "source": [
    "print (news.data[0])\n",
    "print (news.target[0], news.target_names[news.target[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba20218",
   "metadata": {},
   "source": [
    "Build training and testing datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "faa9df44",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "SPLIT_PERC = 0.75\n",
    "split_size = int(len(news.data)*SPLIT_PERC)\n",
    "X_train = news.data[:split_size]\n",
    "X_test = news.data[split_size:]\n",
    "y_train = news.target[:split_size]\n",
    "y_test = news.target[split_size:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee7e61f",
   "metadata": {},
   "source": [
    "This function will serve to perform and evaluate a cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a32c9de7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.81114951 0.79893611 0.79215476 0.80261005 0.80882734]\n",
      "Mean score: 0.803 (+/-0.003)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from scipy.stats import sem\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to evaluate cross-validation scores\n",
    "def evaluate_cross_validation(clf, X, y, K):\n",
    "    # Create a k-fold cross-validation iterator\n",
    "    cv = KFold(n_splits=K, shuffle=True, random_state=0)\n",
    "    \n",
    "    # Calculate cross-validation scores\n",
    "    scores = cross_val_score(clf, X, y, cv=cv)\n",
    "    \n",
    "    print(\"Cross-validation scores:\", scores)\n",
    "    print(\"Mean score: {0:.3f} (+/-{1:.3f})\".format(np.mean(scores), sem(scores)))\n",
    "\n",
    "# Load the California housing dataset\n",
    "california = fetch_california_housing()\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(california.data, california.target, test_size=0.25, random_state=33)\n",
    "\n",
    "# Standardize the features\n",
    "scalerX = StandardScaler().fit(X_train)\n",
    "X_train = scalerX.transform(X_train)\n",
    "X_test = scalerX.transform(X_test)\n",
    "\n",
    "# Initialize the ExtraTreesRegressor\n",
    "clf_et = ExtraTreesRegressor(random_state=42)\n",
    "\n",
    "# Evaluate cross-validation performance\n",
    "evaluate_cross_validation(clf_et, X_train, y_train, K=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a6ebee",
   "metadata": {},
   "source": [
    "Evaluate three models with the same Naive Bayes classifier, but with different vectorizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "38ca927f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating CountVectorizer...\n",
      "Cross-validation scores for CountVectorizer: [0.839052   0.84152812 0.84011319 0.85709232 0.83616419]\n",
      "Mean score: 0.843 (+/- 0.007)\n",
      "\n",
      "Evaluating HashingVectorizer...\n",
      "Cross-validation scores for HashingVectorizer: [0.8800849  0.87477892 0.8623983  0.887867   0.86942675]\n",
      "Mean score: 0.875 (+/- 0.009)\n",
      "\n",
      "Evaluating TfidfVectorizer...\n",
      "Cross-validation scores for TfidfVectorizer: [0.83339229 0.83091617 0.83728334 0.84365051 0.83333333]\n",
      "Mean score: 0.836 (+/- 0.004)\n",
      "\n",
      "Test set score: 0.843\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer, CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Load the 20 newsgroups dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all')\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(newsgroups.data, newsgroups.target, test_size=0.25, random_state=42)\n",
    "\n",
    "# Define the pipelines\n",
    "clf_1 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "clf_2 = Pipeline([\n",
    "    ('vect', HashingVectorizer()),\n",
    "    ('clf', SGDClassifier(random_state=42)),\n",
    "])\n",
    "clf_3 = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "# List of pipelines\n",
    "pipelines = [clf_1, clf_2, clf_3]\n",
    "names = ['CountVectorizer', 'HashingVectorizer', 'TfidfVectorizer']\n",
    "\n",
    "# Evaluate each pipeline using cross-validation\n",
    "for name, pipeline in zip(names, pipelines):\n",
    "    print(f\"Evaluating {name}...\")\n",
    "    scores = cross_val_score(pipeline, X_train, y_train, cv=5)\n",
    "    print(f\"Cross-validation scores for {name}: {scores}\")\n",
    "    print(f\"Mean score: {np.mean(scores):.3f} (+/- {np.std(scores):.3f})\\n\")\n",
    "\n",
    "# Train and test the best pipeline\n",
    "best_pipeline = clf_3  # Assuming TfidfVectorizer performs the best\n",
    "best_pipeline.fit(X_train, y_train)\n",
    "print(f\"Test set score: {best_pipeline.score(X_test, y_test):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a4eb9601",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.85782493 0.85725657 0.84664367 0.85911382 0.8458477 ]\n",
      "Mean score: 0.853 (+/-0.003)\n",
      "Cross-validation scores: [0.86923077 0.88087026 0.8742372  0.88325816 0.88060493]\n",
      "Mean score: 0.878 (+/-0.003)\n",
      "Cross-validation scores: [0.84482759 0.85990979 0.84558238 0.85990979 0.84213319]\n",
      "Mean score: 0.850 (+/-0.004)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean score: 0.850 (+/-0.004)\n"
     ]
    }
   ],
   "source": [
    "clfs = [clf_1, clf_2, clf_3]\n",
    "for clf in clfs:\n",
    "    evaluate_cross_validation(clf, news.data, news.target, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6a1715",
   "metadata": {},
   "source": [
    "We will keep the TF-IDF vectorizer but use a different regular expression to pefrom tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e8d3ee2c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating CountVectorizer...\n",
      "Cross-validation scores for CountVectorizer: [0.839052   0.84152812 0.84011319 0.85709232 0.83616419]\n",
      "Mean score: 0.843 (+/- 0.007)\n",
      "\n",
      "Evaluating HashingVectorizer...\n",
      "Cross-validation scores for HashingVectorizer: [0.8800849  0.87477892 0.8623983  0.887867   0.86942675]\n",
      "Mean score: 0.875 (+/- 0.009)\n",
      "\n",
      "Evaluating TfidfVectorizer...\n",
      "Cross-validation scores for TfidfVectorizer: [0.83339229 0.83091617 0.83728334 0.84365051 0.83333333]\n",
      "Mean score: 0.836 (+/- 0.004)\n",
      "\n",
      "Evaluating Custom TfidfVectorizer...\n",
      "Cross-validation scores for Custom TfidfVectorizer: [0.84824903 0.84400424 0.84966395 0.85921472 0.85244161]\n",
      "Mean score: 0.851 (+/- 0.005)\n",
      "\n",
      "Test set score: 0.859\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer, CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Load the 20 newsgroups dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all')\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(newsgroups.data, newsgroups.target, test_size=0.25, random_state=42)\n",
    "\n",
    "# Define the pipelines\n",
    "clf_1 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "clf_2 = Pipeline([\n",
    "    ('vect', HashingVectorizer()),\n",
    "    ('clf', SGDClassifier(random_state=42)),\n",
    "])\n",
    "clf_3 = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "clf_4 = Pipeline([\n",
    "    ('vect', TfidfVectorizer(\n",
    "                token_pattern=r\"\\b[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+\\b\",\n",
    "    )),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "# List of pipelines\n",
    "pipelines = [clf_1, clf_2, clf_3, clf_4]\n",
    "names = ['CountVectorizer', 'HashingVectorizer', 'TfidfVectorizer', 'Custom TfidfVectorizer']\n",
    "\n",
    "# Evaluate each pipeline using cross-validation\n",
    "for name, pipeline in zip(names, pipelines):\n",
    "    print(f\"Evaluating {name}...\")\n",
    "    scores = cross_val_score(pipeline, X_train, y_train, cv=5)\n",
    "    print(f\"Cross-validation scores for {name}: {scores}\")\n",
    "    print(f\"Mean score: {np.mean(scores):.3f} (+/- {np.std(scores):.3f})\\n\")\n",
    "\n",
    "# Train and test the best pipeline\n",
    "best_pipeline = clf_4  # Assuming the custom TfidfVectorizer performs the best\n",
    "best_pipeline.fit(X_train, y_train)\n",
    "print(f\"Test set score: {best_pipeline.score(X_test, y_test):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c08e67a9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.86100796 0.8718493  0.86203237 0.87291059 0.8588485 ]\n",
      "Mean score: 0.865 (+/- 0.003)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from scipy.stats import sem\n",
    "import numpy as np\n",
    "\n",
    "# Function to evaluate cross-validation scores\n",
    "def evaluate_cross_validation(clf, X, y, K):\n",
    "    # Create a k-fold cross-validation iterator\n",
    "    cv = KFold(n_splits=K, shuffle=True, random_state=0)\n",
    "    \n",
    "    # Calculate cross-validation scores\n",
    "    scores = cross_val_score(clf, X, y, cv=cv)\n",
    "    \n",
    "    print(\"Cross-validation scores:\", scores)\n",
    "    print(\"Mean score: {0:.3f} (+/- {1:.3f})\".format(np.mean(scores), sem(scores)))\n",
    "\n",
    "# Load the 20 newsgroups dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all')\n",
    "\n",
    "# Define the custom pipeline\n",
    "clf_4 = Pipeline([\n",
    "    ('vect', TfidfVectorizer(\n",
    "                token_pattern=r\"\\b[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+\\b\",\n",
    "    )),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "# Evaluate cross-validation performance for clf_4\n",
    "evaluate_cross_validation(clf_4, newsgroups.data, newsgroups.target, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9252b7",
   "metadata": {},
   "source": [
    "Try to improve performance filtering the stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2e203038",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_stop_words():\n",
    "    result = set()\n",
    "    for line in open('stopwords_en.txt', 'r').readlines():\n",
    "        result.add(line.strip())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "416eedb4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "stop_words = get_stop_words()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e2fa157b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.86498674 0.87556381 0.86309366 0.87503317 0.86097108]\n",
      "Mean score: 0.868 (+/- 0.003)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.stats import sem\n",
    "import numpy as np\n",
    "\n",
    "# Function to evaluate cross-validation scores\n",
    "def evaluate_cross_validation(clf, X, y, K):\n",
    "    # Create a k-fold cross-validation iterator\n",
    "    cv = KFold(n_splits=K, shuffle=True, random_state=0)\n",
    "    \n",
    "    # Calculate cross-validation scores\n",
    "    scores = cross_val_score(clf, X, y, cv=cv)\n",
    "    \n",
    "    print(\"Cross-validation scores:\", scores)\n",
    "    print(\"Mean score: {0:.3f} (+/- {1:.3f})\".format(np.mean(scores), sem(scores)))\n",
    "\n",
    "# Load the 20 newsgroups dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all')\n",
    "\n",
    "# Define custom stop words\n",
    "stop_words = ['the', 'and', 'is', 'in', 'it', 'to', 'of']  # Example stop words\n",
    "\n",
    "# Define the custom pipeline\n",
    "clf_5 = Pipeline([\n",
    "    ('vect', TfidfVectorizer(\n",
    "                stop_words=stop_words,\n",
    "                token_pattern=r\"\\b[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+\\b\",\n",
    "    )),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "# Evaluate cross-validation performance for clf_5\n",
    "evaluate_cross_validation(clf_5, newsgroups.data, newsgroups.target, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fbc18c70",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.86498674 0.87556381 0.86309366 0.87503317 0.86097108]\n",
      "Mean score: 0.868 (+/- 0.003)\n"
     ]
    }
   ],
   "source": [
    "evaluate_cross_validation(clf_5, news.data, news.target, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a007c029",
   "metadata": {},
   "source": [
    "Try to improve by adjusting the alpha parameter on the MultinomialNB classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "449d784e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.9198939  0.91987265 0.9169541  0.92544441 0.91801539]\n",
      "Mean score: 0.920 (+/- 0.001)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.stats import sem\n",
    "import numpy as np\n",
    "\n",
    "# Function to evaluate cross-validation scores\n",
    "def evaluate_cross_validation(clf, X, y, K):\n",
    "    # Create a k-fold cross-validation iterator\n",
    "    cv = KFold(n_splits=K, shuffle=True, random_state=0)\n",
    "    \n",
    "    # Calculate cross-validation scores\n",
    "    scores = cross_val_score(clf, X, y, cv=cv)\n",
    "    \n",
    "    print(\"Cross-validation scores:\", scores)\n",
    "    print(\"Mean score: {0:.3f} (+/- {1:.3f})\".format(np.mean(scores), sem(scores)))\n",
    "\n",
    "# Load the 20 newsgroups dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all')\n",
    "\n",
    "# Define custom stop words\n",
    "stop_words = ['the', 'and', 'is', 'in', 'it', 'to', 'of']  # Example stop words\n",
    "\n",
    "# Define the custom pipeline\n",
    "clf_7 = Pipeline([\n",
    "    ('vect', TfidfVectorizer(\n",
    "                stop_words=stop_words,\n",
    "                token_pattern=r\"\\b[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+\\b\",\n",
    "    )),\n",
    "    ('clf', MultinomialNB(alpha=0.01)),\n",
    "])\n",
    "\n",
    "# Evaluate cross-validation performance for clf_7\n",
    "evaluate_cross_validation(clf_7, newsgroups.data, newsgroups.target, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb125bfc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.9204244   0.91960732  0.91828071  0.92677103  0.91854603]\n",
      "Mean score: 0.921 (+/-0.002)\n"
     ]
    }
   ],
   "source": [
    "evaluate_cross_validation(clf_7, news.data, news.target, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6fff3746",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def train_and_evaluate(clf, X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Accuracy on training set:\")\n",
    "    print(clf.score(X_train, y_train))\n",
    "    print(\"Accuracy on testing set:\")\n",
    "    print(clf.score(X_test, y_test))\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(metrics.classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(metrics.confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "020aa45f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set:\n",
      "0.996957690675"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on testing set:\n",
      "0.917869269949"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.88      0.91       216\n",
      "          1       0.85      0.85      0.85       246\n",
      "          2       0.91      0.84      0.87       274\n",
      "          3       0.81      0.86      0.83       235\n",
      "          4       0.88      0.90      0.89       231\n",
      "          5       0.89      0.91      0.90       225\n",
      "          6       0.88      0.80      0.84       248\n",
      "          7       0.92      0.93      0.93       275\n",
      "          8       0.96      0.98      0.97       226\n",
      "          9       0.97      0.94      0.96       250\n",
      "         10       0.97      1.00      0.98       257\n",
      "         11       0.97      0.97      0.97       261\n",
      "         12       0.90      0.91      0.91       216\n",
      "         13       0.94      0.95      0.95       257\n",
      "         14       0.94      0.97      0.95       246\n",
      "         15       0.90      0.96      0.93       234\n",
      "         16       0.91      0.97      0.94       218\n",
      "         17       0.97      0.99      0.98       236\n",
      "         18       0.95      0.91      0.93       213\n",
      "         19       0.86      0.78      0.82       148\n",
      "\n",
      "avg / total       0.92      0.92      0.92      4712\n",
      "\n",
      "Confusion Matrix:\n",
      "[[190   0   0   0   1   0   0   0   0   1   0   0   0   1   0   9   2   0\n",
      "    0  12]\n",
      " [  0 208   5   3   3  13   4   0   0   0   0   1   3   2   3   0   0   1\n",
      "    0   0]\n",
      " [  0  11 230  22   1   5   1   0   1   0   0   0   0   0   1   0   1   0\n",
      "    1   0]\n",
      " [  0   6   6 202   9   3   4   0   0   0   0   0   4   0   1   0   0   0\n",
      "    0   0]\n",
      " [  0   2   3   4 208   1   5   0   0   0   2   0   5   0   1   0   0   0\n",
      "    0   0]\n",
      " [  0   9   2   2   1 205   0   1   1   0   0   0   0   2   1   0   0   1\n",
      "    0   0]\n",
      " [  0   2   3  10   6   0 199  14   1   2   0   1   5   2   2   0   0   1\n",
      "    0   0]\n",
      " [  0   1   1   1   1   0   6 257   4   1   0   0   0   1   0   0   2   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   1   1   2 221   0   0   0   0   1   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   1   0   2 236   5   0   1   3   0   1   1   0\n",
      "    0   0]\n",
      " [  0   0   0   1   0   0   0   0   0   0 256   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   1   0   1   0   0   0 254   0   1   0   0   3   0\n",
      "    1   0]\n",
      " [  0   1   0   1   5   1   3   1   0   2   1   1 197   1   2   0   0   0\n",
      "    0   0]\n",
      " [  0   1   0   1   1   0   0   0   0   0   0   2   2 245   3   0   1   0\n",
      "    0   1]\n",
      " [  0   2   0   0   1   0   0   1   0   0   0   0   0   1 238   0   1   0\n",
      "    1   1]\n",
      " [  1   0   1   2   0   0   0   1   0   0   0   1   1   0   1 225   0   1\n",
      "    0   0]\n",
      " [  0   0   1   0   0   0   1   0   1   0   0   1   0   0   0   0 212   0\n",
      "    2   0]\n",
      " [  0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 234\n",
      "    1   0]\n",
      " [  0   0   0   0   0   0   1   0   0   0   0   2   1   1   0   1   7   3\n",
      "  193   4]\n",
      " [  9   0   0   0   0   1   0   0   0   1   0   0   0   0   0  13   4   1\n",
      "    4 115]]\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(clf_7, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4e034bff",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set:\n",
      "0.99702844205462\n",
      "Accuracy on testing set:\n",
      "0.9227504244482173\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93       198\n",
      "           1       0.80      0.89      0.84       245\n",
      "           2       0.91      0.83      0.87       242\n",
      "           3       0.78      0.84      0.81       238\n",
      "           4       0.90      0.93      0.91       250\n",
      "           5       0.94      0.90      0.92       260\n",
      "           6       0.91      0.80      0.85       241\n",
      "           7       0.94      0.95      0.95       244\n",
      "           8       0.98      0.97      0.97       219\n",
      "           9       0.97      0.99      0.98       261\n",
      "          10       0.98      0.98      0.98       245\n",
      "          11       0.97      0.96      0.97       251\n",
      "          12       0.88      0.89      0.89       249\n",
      "          13       0.97      0.94      0.95       249\n",
      "          14       0.96      0.98      0.97       240\n",
      "          15       0.92      0.99      0.95       245\n",
      "          16       0.91      0.96      0.94       230\n",
      "          17       0.96      0.99      0.97       234\n",
      "          18       0.96      0.91      0.93       207\n",
      "          19       0.94      0.75      0.83       164\n",
      "\n",
      "    accuracy                           0.92      4712\n",
      "   macro avg       0.92      0.92      0.92      4712\n",
      "weighted avg       0.92      0.92      0.92      4712\n",
      "\n",
      "Confusion Matrix:\n",
      "[[188   0   0   0   0   0   0   0   0   0   0   0   0   0   1   2   1   0\n",
      "    1   5]\n",
      " [  0 218   3   5   4   8   2   0   0   0   0   1   0   1   1   0   0   2\n",
      "    0   0]\n",
      " [  0  15 201  16   3   3   2   0   0   0   0   0   2   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   5   8 200   9   0   5   0   0   0   1   1   7   1   0   0   0   0\n",
      "    0   1]\n",
      " [  0   4   2   7 232   0   2   0   0   0   0   1   2   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0  17   0   3   1 234   0   0   1   1   0   0   0   1   1   0   1   0\n",
      "    0   0]\n",
      " [  0   3   1  18   4   0 193   5   2   2   2   0   9   0   1   0   1   0\n",
      "    0   0]\n",
      " [  0   1   0   0   2   0   4 233   0   0   0   0   3   1   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   1   4 212   0   0   0   1   0   0   0   1   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   1   0 258   2   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   1   1   0   0   0   0   1   1 241   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   3   2   0   0   0   0   1   0   0   0 241   1   1   0   0   1   1\n",
      "    0   0]\n",
      " [  0   3   1   6   4   2   4   1   0   1   1   2 221   0   3   0   0   0\n",
      "    0   0]\n",
      " [  0   3   0   2   0   0   0   1   0   0   0   0   4 234   1   0   2   0\n",
      "    2   0]\n",
      " [  0   0   0   0   0   2   0   0   0   0   0   0   0   2 236   0   0   0\n",
      "    0   0]\n",
      " [  1   0   0   0   0   0   0   0   0   0   0   0   0   0   0 243   0   1\n",
      "    0   0]\n",
      " [  0   0   1   0   0   0   0   0   0   1   0   1   0   0   0   0 221   0\n",
      "    4   2]\n",
      " [  1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   2   0 231\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   1   0   0   3   0  11   4\n",
      "  188   0]\n",
      " [ 16   0   0   0   0   0   0   1   0   1   0   0   0   1   0  17   3   1\n",
      "    1 123]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load the 20 newsgroups dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all')\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(newsgroups.data, newsgroups.target, test_size=0.25, random_state=42)\n",
    "\n",
    "# Define custom stop words\n",
    "stop_words = ['the', 'and', 'is', 'in', 'it', 'to', 'of']  # Example stop words\n",
    "\n",
    "# Define the custom pipeline\n",
    "clf_7 = Pipeline([\n",
    "    ('vect', TfidfVectorizer(\n",
    "                stop_words=stop_words,\n",
    "                token_pattern=r\"\\b[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+\\b\",\n",
    "    )),\n",
    "    ('clf', MultinomialNB(alpha=0.01)),\n",
    "])\n",
    "\n",
    "# Train and evaluate the classifier\n",
    "train_and_evaluate(clf_7, X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "10150a4b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149988\n"
     ]
    }
   ],
   "source": [
    "# Print the number of features\n",
    "print(len(clf_7.named_steps['vect'].get_feature_names_out()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
